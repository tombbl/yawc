v0.1
====
1. Wczytaj stronę.

2. Adres strony pobierz z wiersza poleceń (argument podawany przy wywołaniu).

3. Wyświetl wszystkie linki dostępne na stronie w formacie: tekst linku => adres linku.
   Jeśli link nie ma tekstu, ani tekstu alternatywnego, wyświetl ciąg NO_TEXT_LINK.

4. Wyświetl podsumowanie na końcu listy: 
   o ile linków łącznie znaleziono
   o ile łącz prowadzi do innych stron
   o ile łącz w obrębie analizowanej strony

1 - completed
2 - completed
3 - modified & completed
4 - to be done


v0.2
====
1. Crawler ma odwiedzać tylko te linki, których jeszcze nie odwiedzał.
   Obecnie, zdarzają się jeszcze sytuacje, że pojawia się komunikat "The page has been visited. Skipping".
   Nie powinno to mieć już miejsca, ponieważ jest już kod do weryfikacji linków. Gdzieś jest bug.

2. Dodać opcję - na razie nie pobieraną z linii komend - za pomocą, której ustawia się głębokość skanowania stron.
   0 - bez ograniczeń
   1 - skanowanie w obrębie domeny
   2 - crawler skanuje zadaną domenę oraz całą kolejną domenę zewnętrzną napotkaną jako pierwszą
   3 - crawler skanuje zadaną domenę i całe 2 domeny zewnętrzne, napotkane jako pierwsze
   4 - j.w.
   5 - j.w.
   Ustalić wartość domyślną. 1?

3. Dodać możliwość czystego zakończenia programu - yawc ma wiedzieć, że user zażądał zakończenia
   wykonywania programu, wyświetlić stosowny komunikat i zakończyć działanie bez informacji o błędach.
   (Ctrl+c generuje komunikat o błędzie)


More ideas to be implemented:
1. Start using english only
2. Add option to scan strictly within a given host
3. Give multiple adresses to be scanned as cli input
4. Write unique links to the output file given by the user
5. Read links to be scanned from file given by user
6. Start using a documentation tool.
7. On user request respect robots.txt file when visiting a site.




