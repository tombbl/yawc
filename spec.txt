v0.1
====
1. Read the page.

2. Get site url from the command line (as an argument when invoking the script).

3. Display all the links addresses available on the given page.

4. At the end display an information how many unique links has been found.

1 - completed
2 - completed
3 - completed
4 - completed


v0.2
====
5. yawc has to visit only those links which were not visited in the past.
   In current code revision it happens sporadically that a message appears "The page has been visited. Skipping".
   It should not happen, as the code to remove duplicate links is there. Find the bug.

6. Add an option to set a depth of recursive sites scanning.
   0 - no limits
   1 - scan within the given domain
   2 - scan the given domain and another one encountered as first in a row
   3 - scan the given domain and 2 another ones, which were encountered as first in a row
   4 - etc.
   Set the default value. 1?

7. Give the user possibility to terminate the script in a clean manner -> yawc has to:
   o know that the user requested to finish scanning
   o display a suitable message
   o terminate without any failures messages

5 - completed - extended String class with 4 new methods: unique, unique!, unique_with(array_to_compare_to), unique_with!(array_to_compare_to)
                It works nicely and speed of this solution is - at this level - acceptable.
6 - running
7 - completed - user can terminate yawc by hitting Ctrl+c

v0.3
====
8. Stop using relative URLs. If a relative URL is encountered - extend it into an absolute URL.


More ideas to be implemented:
1. Start using english only
2. Add option to scan strictly within a given host
3. Give multiple adresses to be scanned as cli input
4. Write unique links to the output file given by the user
5. Read links to be scanned from file given by user
6. Start using a documentation tool.
7. On user request respect robots.txt file when visiting a site.
8. Store the final links in the NoSQL database (Redis?)
